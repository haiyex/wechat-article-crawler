# 微信公众号文章抓取工具 - 开发文档

## 1. 项目架构

### 1.1 整体架构
```
微信公众号文章抓取工具
├── 爬虫模块 (Crawler)
│   ├── 搜狗搜索API调用
│   └── 文章列表获取
├── 解析模块 (Parser)
│   ├── 文章元数据提取
│   └── 正文内容解析
├── 下载模块 (Downloader)
│   ├── 图片下载
│   └── 并发控制
├── 存储模块 (Storage)
│   ├── Markdown文件生成
│   └── 文件目录管理
└── 命令行模块 (CLI)
    ├── 参数解析
    └── 进度显示
```

### 1.2 项目结构
```
z_ai/
├── wechat_crawler/          # 主程序目录
│   ├── __init__.py
│   ├── crawler.py          # 爬虫模块
│   ├── parser.py           # 解析模块
│   ├── downloader.py       # 下载模块
│   ├── storage.py          # 存储模块
│   └── cli.py              # 命令行入口
├── config.py                # 配置文件
├── requirements.txt         # 依赖包
├── setup.py                 # 安装脚本
└── README.md               # 使用说明
```

## 2. 技术选型

### 2.1 核心库
- **requests**: HTTP请求库，用于访问搜狗搜索API和下载图片
- **beautifulsoup4**: HTML解析库，用于解析文章内容
- **lxml**: BeautifulSoup的解析器，性能更好
- **wechatsogou**: 搜狗微信搜索API封装库（如果有问题可改用requests直接调用）

### 2.2 辅助库
- **click**: 命令行参数解析库
- **tqdm**: 进度条显示
- **aiohttp/asyncio**: 异步IO，用于并发下载图片

## 3. 模块设计

### 3.1 爬虫模块 (crawler.py)

#### 3.1.1 WechatCrawler类
```python
class WechatCrawler:
    def __init__(self, config):
        pass

    def get_account_articles(self, account_name, limit=None):
        """获取公众号文章列表"""

    def get_article_content(self, article_url):
        """获取文章详细内容"""
```

#### 3.1.2 功能说明
- `get_account_articles`: 通过搜狗搜索API获取公众号文章列表
  - 输入：公众号名称，可选文章数量限制
  - 输出：文章列表（包含标题、链接、日期等）

- `get_article_content`: 获取文章HTML内容
  - 输入：文章URL
  - 输出：HTML文本

### 3.2 解析模块 (parser.py)

#### 3.2.1 ArticleParser类
```python
class ArticleParser:
    @staticmethod
    def parse_article_meta(html):
        """解析文章元数据"""

    @staticmethod
    def parse_article_content(html):
        """解析文章正文内容"""

    @staticmethod
    def extract_images(html):
        """提取图片链接"""
```

#### 3.2.2 功能说明
- `parse_article_meta`: 从HTML中提取标题、发布日期、作者等信息
- `parse_article_content`: 提取正文内容，保持格式
- `extract_images`: 提取所有图片的URL链接

### 3.3 下载模块 (downloader.py)

#### 3.3.1 ImageDownloader类
```python
class ImageDownloader:
    def __init__(self, output_dir, max_workers=5):
        pass

    def download_image(self, url, filename):
        """下载单张图片"""

    def download_images_async(self, image_list):
        """并发下载多张图片"""
```

#### 3.3.2 功能说明
- 支持单张图片下载
- 支持异步并发下载
- 自动处理图片重命名
- 错误重试机制

### 3.4 存储模块 (storage.py)

#### 3.4.1 ArticleStorage类
```python
class ArticleStorage:
    def __init__(self, base_dir, account_name):
        pass

    def save_article(self, article_data):
        """保存文章为Markdown文件"""

    def _generate_markdown(self, article_data):
        """生成Markdown内容"""

    def _sanitize_filename(self, filename):
        """清理文件名中的非法字符"""
```

#### 3.4.2 功能说明
- 创建目录结构
- 生成格式化的Markdown文件
- 处理文件名特殊字符
- 图片路径引用管理

### 3.5 命令行模块 (cli.py)

#### 3.5.1 main函数
```python
@click.command()
@click.argument('account_name')
@click.option('--output', '-o', default='./output', help='输出目录')
@click.option('--limit', '-l', default=None, help='文章数量限制')
def main(account_name, output, limit):
    """主入口函数"""
```

#### 3.5.2 功能说明
- 解析命令行参数
- 协调各模块工作
- 显示进度条
- 错误处理和提示

## 4. 数据结构设计

### 4.1 文章数据结构
```python
@dataclass
class Article:
    title: str              # 文章标题
    url: str                # 文章链接
    publish_date: str       # 发布日期
    author: str             # 作者
    content: str            # 正文内容（Markdown格式）
    images: List[Dict]      # 图片列表 [{'url': '', 'local_path': ''}]
```

### 4.2 配置数据结构
```python
@dataclass
class Config:
    timeout: int = 30       # 请求超时时间
    retry: int = 3          # 重试次数
    max_workers: int = 5    # 并发下载数
    user_agent: str         # User-Agent
```

## 5. 实现步骤

### 阶段一：环境搭建
1. 创建项目目录结构
2. 创建虚拟环境
3. 安装依赖包
4. 编写配置文件

### 阶段二：核心模块开发
1. 实现爬虫模块（crawler.py）
   - 搜狗搜索API集成
   - 文章列表获取
   - 文章内容获取

2. 实现解析模块（parser.py）
   - 元数据提取
   - 正文内容解析
   - 图片链接提取

3. 实现下载模块（downloader.py）
   - 单图片下载
   - 异步并发下载

4. 实现存储模块（storage.py）
   - Markdown文件生成
   - 目录管理
   - 文件名处理

### 阶段三：命令行集成
1. 实现CLI模块（cli.py）
2. 参数解析和验证
3. 进度显示
4. 错误处理

### 阶段四：测试和优化
1. 单元测试
2. 集成测试
3. 性能优化
4. 错误处理完善

## 6. 关键技术实现

### 6.1 搜狗搜索API使用
- 使用wechatsogou库或直接调用搜狗搜索
- 处理反爬虫机制（User-Agent、Cookie等）
- 分页获取文章列表

### 6.2 图片处理
- 识别图片类型（JPG、PNG、GIF等）
- 处理相对路径和绝对路径
- 使用hash生成唯一文件名

### 6.3 Markdown生成
- 保持HTML格式转换为Markdown
- 处理标题、段落、列表等格式
- 图片链接转换

### 6.4 并发控制
- 使用asyncio进行异步IO
- 控制并发数量，避免被封IP
- 实现请求限流

## 7. 错误处理策略

### 7.1 网络错误
- 请求超时重试
- 网络异常捕获
- 失败日志记录

### 7.2 数据错误
- HTML解析失败处理
- 必需字段缺失处理
- 图片下载失败处理

### 7.3 系统错误
- 磁盘空间不足
- 文件权限问题
- 目录创建失败

## 8. 测试计划

### 8.1 单元测试
- 爬虫模块测试
- 解析模块测试
- 下载模块测试
- 存储模块测试

### 8.2 集成测试
- 完整流程测试
- 边界条件测试
- 异常情况测试

### 8.3 性能测试
- 大量文章抓取测试
- 并发下载性能测试

## 9. 部署和使用

### 9.1 安装
```bash
pip install -r requirements.txt
```

### 9.2 使用
```bash
python -m wechat_crawler.cli 公众号名称 -o ./output
```

### 9.3 参数说明
- `account_name`: 公众号名称（必填）
- `-o, --output`: 输出目录（默认：./output）
- `-l, --limit`: 文章数量限制（可选）

## 10. 后续优化方向

- 支持断点续传
- 支持增量更新
- 添加数据库存储选项
- 支持视频下载
- Web界面支持
